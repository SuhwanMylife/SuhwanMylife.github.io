---
layout: post
title: "앙상블의 종류 / 블렌딩이란?"
---

<br>

시계열 예측 목적으로 이런 자료들을 보고 있는데, 특히 스태킹은 시계열 예측과는 맞지 않는 것 같아 일단 보류한다. 추후에 앙상블을 사용한 사례가 등장하면 추가적으로 정리해 볼 예정이다.

<br>

**Voting의 종류: 하드 보팅(Hard voting), 소프트 보팅(Soft voting)**

# 하드 보팅 Hard voting

각 모델들의 예측결과값을 바탕으로 다수결 투표하는 방식

<br>

# 소프트 보팅 Soft voting

모델들의 예측 확률값의 평균 또는 가중치 합을 사용하는 방식

가중치 부여 방식: 임의로 부여, 스태킹(Stacking) 기법 활용하여 부여

<br>

앙상블 종류: 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)

# 배깅 Bagging

Bagging = Bootstrap Aggregating의 약자. 부트스트랩(Bootstrap)을 이용한다.

부트스트랩: 주어진 데이터셋에서 random sampling하여 새로운 dataset을 만들어내는 것을 의미

- 부트스트랩을 통해 만들어진 여러 데이터셋을 바탕으로 weak learner를 훈련시킨 뒤 결과를 voting
- 예시: Random Forest

<br>

# 부스팅 Boosting

부스팅: 반복적으로 모델을 업데이트. 이 때, 이전 iteration의 결과에 따라 dataset sample에 대한 가중치 부여

⇒ 반복할 때마다 각 샘플의 중요도에 따라 다른 분류기가 생성됨

⇒ 최종적으로 모든 iteration에서 생성된 모델의 결과를 voting

![Untitled](https://github.com/SuhwanMylife/SuhwanMylife.github.io/assets/70688382/02a31369-7ea2-4100-854d-04fb8d4feb68)

Iteration 1의 빨간 원은 잘못 분류된 샘플. 이 샘플에 대한 가중치를 높여 다시 분류기를 만든다.

Iteration 2에서 Iteration 1의 분류기와 새로운 분류기를 함께 사용하여 분류. 그 결과 파란 사각형이 잘못 분류됨. 이 샘플에 대한 가중치 높여 다시 분류기 생성.

이런 식으로 계속 반복하여 최종적으로 만들어진 분류기들을 모두 결합하여 최종 모델 생성

Boosting은 다시 Adaptive Boosting(AdaBoost)와 Gradient Boosting Model(GBM) 계열로 나눌 수 있다.

- AdaBoost
- Gradient Boosting
- XGBoost
- LightGBM
- CatBoost
- NGBoost

<br>

# 스태킹 Stacking

weak learner들의 예측 결과를 바탕으로 meta learner로 학습시켜 최종 예측값을 결정하는 방법

- meta learner 또한 학습이 필요, 이 때 사용되는 데이터는 training data에 대한 각 weak learner들의 예측 확률값의 모음
- 기본적인 스태킹 앙상블은 과적합 문제가 있어 사용하지 않음. 보통 CV 기반(KFold 등)의 stacking ensemble을 사용

<br>

참고자료

[머신러닝 스태킹 앙상블(stacking ensemble) 이란? - 스태킹 앙상블 기본편(stacking ensemble basic)](https://lsjsj92.tistory.com/558)

[머신러닝 스태킹 앙상블(stacking ensemble)이란? - CV(Kfold) 기반 stacking ensemble](https://lsjsj92.tistory.com/559)

<br>

참고자료

[https://tyami.github.io/machine learning/ensemble-1-basics/](https://tyami.github.io/machine%20learning/ensemble-1-basics/)